{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998fbc9b",
   "metadata": {},
   "source": [
    "This dataset is created using the template for creating a dataset from scratch as in: https://cookiecutter-easydata.readthedocs.io/en/latest/New-Dataset-Template/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd47272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:56,860 - utils - INFO - NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "# Basic utility functions\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "\n",
    "from src.log import logger\n",
    "from src import paths\n",
    "from src.utils import list_dir\n",
    "from functools import partial\n",
    "\n",
    "# data functions\n",
    "from src.data import DataSource, Dataset, DatasetGraph, Catalog\n",
    "from src import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab2e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally set to debug log level\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e22eea",
   "metadata": {},
   "source": [
    "## Create the Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7faedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'penguins-raw'\n",
    "dsrc = DataSource(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1cb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43611ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'penguins_size.csv' # path relative to paths['raw_data_path'] for the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d90b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "license = \"\"\"\n",
    "Data are available by [CC-0](https://github.com/allisonhorst/palmerpenguins) license in accordance with the Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86edc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"\"\"\n",
    "The goal of palmerpenguins is to provide a great dataset for data exploration & visualization, as an alternative to `iris`.\n",
    "\n",
    "More information can be found at [https://github.com/allisonhorst/palmerpenguins](https://github.com/allisonhorst/palmerpenguins).\n",
    "\n",
    "Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n",
    "\n",
    "The data consists of measurements of bill (culmen) and flippers and weights of three species of penguins, along with some other metadata about the penguins. In total we have 334 different penguins measured.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba3994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.add_url(url=url, file_name=filename, unpack_action='copy')\n",
    "dsrc.add_metadata(contents=metadata, force=True)\n",
    "dsrc.add_metadata(contents=license, kind='LICENSE', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136930ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.extra import process_extra_files\n",
    "process_function = process_extra_files\n",
    "process_function_kwargs = {'file_glob':'*.csv',\n",
    "                           'do_copy': True,\n",
    "                           'extra_dir': ds_name+'.extra',\n",
    "                           'extract_dir': ds_name}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b6bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.process_function = partial(process_function, **process_function_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eac4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:57,419 - catalog - DEBUG - Loaded 2 records from 'datasources' Catalog.\n",
      "2021-10-26 10:12:57,425 - catalog - DEBUG - Verifying serialization for catalog 'datasources'\n",
      "2021-10-26 10:12:57,436 - catalog - DEBUG - Writing entry:'penguins-raw' to catalog:'datasources'.\n",
      "2021-10-26 10:12:57,439 - datasets - DEBUG - Updated datasource:penguins-raw in catalog\n"
     ]
    }
   ],
   "source": [
    "dsrc.update_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd14fa7",
   "metadata": {},
   "source": [
    "## Create the Corresponding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f14c33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DatasetGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba4b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:57,477 - catalog - DEBUG - Loaded 3 records from 'transformers' Catalog.\n",
      "2021-10-26 10:12:57,478 - catalog - DEBUG - Verifying serialization for catalog 'transformers'\n",
      "2021-10-26 10:12:57,492 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:57,495 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n",
      "2021-10-26 10:12:57,502 - datasets - DEBUG - Loaded DatasetGraph with 3 nodes and 3 edges.\n"
     ]
    }
   ],
   "source": [
    "dag = DatasetGraph(catalog_path=paths['catalog_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d4a1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:57,519 - catalog - DEBUG - Writing entry:'_penguins-raw' to catalog:'transformers'.\n",
      "2021-10-26 10:12:57,523 - datasets - INFO - Regenerating output Dataset 'penguins-raw' and adding to catalog\n",
      "2021-10-26 10:12:57,526 - datasets - DEBUG - Generating edge traversal list for Dataset:'penguins-raw'\n",
      "2021-10-26 10:12:57,527 - datasets - DEBUG - traverse: examining vertex:'penguins-raw'\n",
      "2021-10-26 10:12:57,529 - datasets - DEBUG - traverse: all input dependencies:[] satisfied for edge: '_penguins-raw'\n",
      "2021-10-26 10:12:57,531 - datasets - DEBUG - Traversal complete. Edges to process: ['_penguins-raw']\n",
      "2021-10-26 10:12:57,533 - datasets - DEBUG - process_edge: Processing input datasets for edge:'_penguins-raw'\n",
      "2021-10-26 10:12:57,535 - datasets - DEBUG - process_edge:Applying transformer: {'transformer_module': 'src.data.datasets', 'transformer_name': 'dataset_from_datasource', 'transformer_kwargs': {'dataset_name': 'penguins-raw', 'datasource_name': 'penguins-raw'}} to input datasets: []\n",
      "2021-10-26 10:12:57,544 - catalog - DEBUG - Loaded 2 records from 'datasources' Catalog.\n",
      "2021-10-26 10:12:57,546 - catalog - DEBUG - Verifying serialization for catalog 'datasources'\n",
      "2021-10-26 10:12:57,555 - fetch - DEBUG - penguins_size.csv already exists. Checking hash...\n",
      "2021-10-26 10:12:57,559 - fetch - DEBUG - penguins_size.csv exists, but no hash to check. Setting to sha1:24ec9bd3af138329a4dcd98f1756ea46bbe0d9a5\n",
      "2021-10-26 10:12:57,569 - fetch - DEBUG - Generating penguins-raw.readme hash...\n",
      "2021-10-26 10:12:57,581 - fetch - DEBUG - Generating penguins-raw.license hash...\n",
      "2021-10-26 10:12:57,595 - fetch - DEBUG - Copying penguins_size.csv...\n",
      "2021-10-26 10:12:57,607 - fetch - DEBUG - Copying penguins-raw.readme...\n",
      "2021-10-26 10:12:57,633 - fetch - DEBUG - Copying penguins-raw.license...\n",
      "2021-10-26 10:12:57,668 - extra - DEBUG - Do copy: True\n",
      "2021-10-26 10:12:57,669 - extra - WARNING - Cleaning contents of penguins-raw.extra\n",
      "2021-10-26 10:12:57,673 - extra - DEBUG - Copying files to /Users/amywooding/easydata/easydata-tutorial/data/processed/penguins-raw.extra...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c33097882f49faa5a52fe0405f3dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:57,772 - datasets - INFO - Generated output datasets: ['penguins-raw'] via edge:'_penguins-raw'\n",
      "2021-10-26 10:12:57,781 - datasets - DEBUG - Updating hashes for dataset 'penguins-raw': {'hashes': {'data': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b', 'target': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'}}.\n",
      "2021-10-26 10:12:57,782 - datasets - DEBUG - process_edge: Updating catalog entry for penguins-raw\n",
      "2021-10-26 10:12:57,784 - catalog - DEBUG - Writing entry:'penguins-raw' to catalog:'datasets'.\n",
      "2021-10-26 10:12:57,789 - datasets - DEBUG - process_edge: Overwriting 'penguins-raw' in `dataset_path`\n",
      "2021-10-26 10:12:57,792 - datasets - DEBUG - Updating hashes for dataset 'penguins-raw': {'hashes': {'data': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b', 'target': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'}}.\n",
      "2021-10-26 10:12:57,799 - datasets - DEBUG - Wrote Dataset Metadata: penguins-raw.metadata\n",
      "2021-10-26 10:12:57,804 - datasets - DEBUG - Re-scanning Dataset catalog before update\n",
      "2021-10-26 10:12:57,810 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:57,820 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n",
      "2021-10-26 10:12:57,828 - catalog - DEBUG - Writing entry:'penguins-raw' to catalog:'datasets'.\n",
      "2021-10-26 10:12:57,831 - datasets - DEBUG - Updated dataset catalog with 'penguins-raw' metadata\n",
      "2021-10-26 10:12:57,835 - datasets - DEBUG - Wrote Dataset: penguins-raw.dataset\n",
      "2021-10-26 10:12:57,842 - datasets - DEBUG - process_edge: Reloading Dataset catalog after processing edge:'_penguins-raw'\n",
      "2021-10-26 10:12:57,847 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:57,849 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_penguins-raw': {'transformations': [{'transformer_module': 'src.data.datasets',\n",
       "    'transformer_name': 'dataset_from_datasource',\n",
       "    'transformer_kwargs': {'dataset_name': 'penguins-raw',\n",
       "     'datasource_name': 'penguins-raw'}}],\n",
       "  'output_datasets': ['penguins-raw']}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag.add_source(output_dataset=ds_name, datasource_name=ds_name, overwrite_catalog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ca4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:57,922 - catalog - DEBUG - Loaded 3 records from 'transformers' Catalog.\n",
      "2021-10-26 10:12:57,930 - catalog - DEBUG - Verifying serialization for catalog 'transformers'\n",
      "2021-10-26 10:12:57,934 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:57,936 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n",
      "2021-10-26 10:12:57,948 - datasets - DEBUG - Loaded DatasetGraph with 3 nodes and 3 edges.\n",
      "2021-10-26 10:12:57,950 - datasets - DEBUG - Generating edge traversal list for Dataset:'penguins-raw'\n",
      "2021-10-26 10:12:57,953 - datasets - DEBUG - traverse: examining vertex:'penguins-raw'\n",
      "2021-10-26 10:12:57,956 - datasets - DEBUG - traverse: all input dependencies:[] satisfied for edge: '_penguins-raw'\n",
      "2021-10-26 10:12:57,961 - datasets - DEBUG - Traversal complete. Edges to process: ['_penguins-raw']\n",
      "2021-10-26 10:12:57,962 - datasets - DEBUG - process_edge: Processing input datasets for edge:'_penguins-raw'\n",
      "2021-10-26 10:12:57,971 - datasets - DEBUG - process_edge:Applying transformer: {'transformer_kwargs': {'dataset_name': 'penguins-raw', 'datasource_name': 'penguins-raw'}, 'transformer_module': 'src.data.datasets', 'transformer_name': 'dataset_from_datasource'} to input datasets: []\n",
      "2021-10-26 10:12:57,982 - catalog - DEBUG - Loaded 2 records from 'datasources' Catalog.\n",
      "2021-10-26 10:12:57,986 - catalog - DEBUG - Verifying serialization for catalog 'datasources'\n",
      "2021-10-26 10:12:57,997 - fetch - DEBUG - penguins_size.csv already exists. Checking hash...\n",
      "2021-10-26 10:12:58,011 - fetch - DEBUG - penguins_size.csv exists, but no hash to check. Setting to sha1:24ec9bd3af138329a4dcd98f1756ea46bbe0d9a5\n",
      "2021-10-26 10:12:58,018 - fetch - DEBUG - Generating penguins-raw.readme hash...\n",
      "2021-10-26 10:12:58,024 - fetch - DEBUG - Generating penguins-raw.license hash...\n",
      "2021-10-26 10:12:58,035 - fetch - DEBUG - Copying penguins_size.csv...\n",
      "2021-10-26 10:12:58,040 - fetch - DEBUG - Copying penguins-raw.readme...\n",
      "2021-10-26 10:12:58,048 - fetch - DEBUG - Copying penguins-raw.license...\n",
      "2021-10-26 10:12:58,070 - extra - DEBUG - Do copy: True\n",
      "2021-10-26 10:12:58,076 - extra - WARNING - Cleaning contents of penguins-raw.extra\n",
      "2021-10-26 10:12:58,079 - extra - DEBUG - Copying files to /Users/amywooding/easydata/easydata-tutorial/data/processed/penguins-raw.extra...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1828efd704144a522d7c8bcfb5d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:58,165 - datasets - INFO - Generated output datasets: ['penguins-raw'] via edge:'_penguins-raw'\n",
      "2021-10-26 10:12:58,170 - datasets - DEBUG - Updating hashes for dataset 'penguins-raw': {'hashes': {'data': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b', 'target': 'sha1:38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'}}.\n",
      "2021-10-26 10:12:58,171 - datasets - DEBUG - process_edge: Reloading Dataset catalog after processing edge:'_penguins-raw'\n",
      "2021-10-26 10:12:58,173 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:58,175 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_catalog(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3791e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:12:58,195 - catalog - DEBUG - Loaded 3 records from 'transformers' Catalog.\n",
      "2021-10-26 10:12:58,196 - catalog - DEBUG - Verifying serialization for catalog 'transformers'\n",
      "2021-10-26 10:12:58,211 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:58,212 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n",
      "2021-10-26 10:12:58,215 - datasets - DEBUG - Loaded DatasetGraph with 3 nodes and 3 edges.\n",
      "2021-10-26 10:12:58,217 - datasets - DEBUG - Verifying hashes using Dataset catalog.\n",
      "2021-10-26 10:12:58,220 - catalog - DEBUG - Loaded 3 records from 'datasets' Catalog.\n",
      "2021-10-26 10:12:58,221 - catalog - DEBUG - Verifying serialization for catalog 'datasets'\n",
      "2021-10-26 10:12:58,224 - datasets - DEBUG - Load penguins-raw from disk...\n",
      "2021-10-26 10:12:58,228 - datasets - DEBUG - Loaded penguins-raw from disk.\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0847a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The goal of palmerpenguins is to provide a great dataset for data exploration & visualization, as an alternative to `iris`.\n",
      "\n",
      "More information can be found at [https://github.com/allisonhorst/palmerpenguins](https://github.com/allisonhorst/palmerpenguins).\n",
      "\n",
      "Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n",
      "\n",
      "The data consists of measurements of bill (culmen) and flippers and weights of three species of penguins, along with some other metadata about the penguins. In total we have 334 different penguins measured.\n"
     ]
    }
   ],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f639735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penguins-raw.extra': {'penguins_size.csv': ['size:13525']}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29e595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:easydata-tutorial] *",
   "language": "python",
   "name": "conda-env-easydata-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
