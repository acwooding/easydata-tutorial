{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370c2a77",
   "metadata": {},
   "source": [
    "# Stage 4: Where do I find the data?\n",
    "\n",
    "This is data science and data analysis after all. We need data. And when we have data, we need to know what data we have. Here are the big questions around finding and prepping data:\n",
    "* Where do I find the data?\n",
    "* How can I use the data?\n",
    "* How do I know that I have the right data?\n",
    "* **BONUS** How can I make it easy to retrieve and use again? (Without copy and pasting code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb8b05",
   "metadata": {},
   "source": [
    "### Imagine: a hypothetical workflow for downloaded data \n",
    "(Assuming you're using reproducible environments already!)\n",
    "\n",
    "1. **Downloading data**: First, you look for a dataset to use. You get one. You download the data, or you copy and paste a script you find off of the internet for how to download the data. \n",
    "2. **Data prep**: Then you copy and paste the munging script that you saw for how to get it into a pandas dataframe.\n",
    "3. **Data prep-2**: You try some analysis, but it doesn't quite work, you need to clean the data up a bit more, so you go back to doing some more prep, this time, your own way. \n",
    "4. **Analysis**: Finally, you can run your analysis and do some data science work. \n",
    "5. ...time passes...\n",
    "6. **Re-downloading**: You mention the result and a colleague asks you about the work, so you go back to dig up the code. You decide to see if you can share your work. You start to run your code, but then realize that the dataset has been updated. It's not clear what's been changed, but you notice in the output that there seem to be more entries in the dataframe. When your script downloaded it, you replaced the previous dataset with the new one. \n",
    "7. **Checking licenses**: When you go to try and figure out what happened, and start looking up info on the dataset. The source you used for information the first time doesn't seem to be up-to-date so you keep digging until you find the original source. In the process, you realize you forgot to look at the license. Better check on that before sharing your work. Thankfully, it's CC-BY-NC, and you're not using it for work purposes. You take a sigh of relief, as this could have been a show-stopper, and you continue.\n",
    "8. **Data prep-3**: You can't roll back to the old data, and the new data has some new gotchas, so you spend some time trying to sort that out. Finally you have something that you can re-run your analysis on, and share with your colleague. \n",
    "9. **Analysis-2**: Fingers crossed with the new larger dataset that things work our roughly the same. There's no easy way now to figure out what's going on if the results look different from before...\n",
    "10. **Sharing**: Phew. Wiping the sweat from your brow, things look roughly similar. You can pass on the work to your colleague.\n",
    "\n",
    "In the hypothetical (or not...this may be pulling from personal experience) situation above, getting to the analysis stage the second time around took just as long, if not longer because of trying to figure out what had changed about the dataset, and tracking down the dataset information and licenses, which were no longer as easy to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04315afb",
   "metadata": {},
   "source": [
    "### Some Reproducibility issues\n",
    "In the previous workflow, here were some of the issues that made it hard to reproduce:\n",
    "\n",
    "* **NO-DATA-LICENSE**: No data license with the data. In fact, we forgot to even check the first time. It wasn't a showstopper, this time, but it could have easily been.\n",
    "* **NO-DATA-METADATA**: No metadata with the data. We didn't have a description of what the data was in a way that made it easy to see what the data was. And in this case, what might have changed with the new metadata.\n",
    "* **NO-DATA-HASH**: No data hash. This meant that there was no way to catch early on that the data was different. \n",
    "* **COOKED-DATA**: Data gets cooked when it gets changed. In this case, the change was downloading the new data on top of the old. Most frequently, we see this happening when data that gets preped gets saved over the raw data. Repeat after me: \"Data is immutable\".\n",
    "* **BLACK-MAGIC**: The data prep was a form of black magic. In this example, we copied and pasted, and we didn't pay attention to what was inside it, it just worked the first time. In particular, the download script had a hard-coded path that the download went to and it would redownload the file into the same place every time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700014e",
   "metadata": {},
   "source": [
    "## The Easydata Way\n",
    "\n",
    "### Default Better Principles\n",
    "\n",
    "* **Data and metadata always stay together**: Keep your data and your metadata together at all times. This includes licenses and hashes. \n",
    "* **Keep a hash of your data and check it**: This is especially true for your raw data, your starting points from which the rest of your data is derived.\n",
    "* **Data is immutable**: Never edit a raw data file. Especially not manually. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. \n",
    "* **Keep (easy-to-use) data recipes**: The code you write should move the raw data through a pipeline to your final analysis. Keep data recipes as a way of recovering your data from its raw format. Your data recipe ideally has an easy-to-use API that gives people you're sharing with a *common start line* when working with the data from a relatively cleaned up state.\n",
    "\n",
    "### How it's done in Easydata\n",
    "\n",
    "Now we'll take a look at how we do this in Easydata. First, some imports that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2282502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215105d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "import logging\n",
    "from src.log import logger\n",
    "from src.helpers import notebook_as_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b732d7",
   "metadata": {},
   "source": [
    "### Set to debug log level\n",
    "Let's set the log level on to DEBUG on the Easydata src module to see what the Dataset code is doing under the hood..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509b992",
   "metadata": {},
   "source": [
    "## Dataset objects\n",
    "\n",
    "#### Default Better Principle: Data and metadata always stay together\n",
    "The Dataset object has metadata attached to it. By **default** Dataset objects keep their metadata with them at all times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load(\"dataset-challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b10dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26fc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fb3e7",
   "metadata": {},
   "source": [
    "Penguin data\n",
    "------------\n",
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png\" alt=\"Penguins\" width=400px/>\n",
    "\n",
    "Time to try it out. The next step is to get some data to work with. To ease us into things\n",
    "we'll start with the [penguin\n",
    "dataset](https://github.com/allisonhorst/penguins). It isn't very\n",
    "representative of what real data would look like, but it is small both\n",
    "in number of points and number of features, and will let us quickly and easily get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019c715",
   "metadata": {},
   "source": [
    "## Load the pre-created a Dataset recipe\n",
    "\n",
    "Here we use the dataset recipe to get the raw penguins dataset. When running \n",
    "\n",
    "```ds = Dataset.load('penguins-raw')```\n",
    "\n",
    "Easydata uses an entry in the Dataset Catalog to find the `penguins-raw` Dataset recipe.\n",
    "\n",
    "Here's what it will do:\n",
    "* Look in the Catalog for the entry `penguins-raw`\n",
    "* Traceback through the dependency chain of datasets looking for what's already cashed and matches the hash check, and what needs to be created from scratch\n",
    "* If this is your first time running, it will download the dataset and created it the first time, populating the `data/raw` directory with the raw download\n",
    "* Do a hash check\n",
    "* Create the `penguins-raw` Dataset\n",
    "* If successful, it will save a processed version of the data in `data/processed` for quick and easy loading any time\n",
    "\n",
    "Say you need more space on your machine. You can blow away the contents of your `data/processed` directory at any time, and `.load(\"penguins-raw\")` will recreate the Dataset again from `data/raw` whenever you need to use it.\n",
    "\n",
    "**BONUS** Beyond the scope of this tutorial, your `data/raw` directory doesn't have to be local and can be kept on a bigger machine or even in your cloud storage (AWS, Azure, etc.). This can be handled by the local paths structure introduced in `02-Environment-Challenge`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff925e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('penguins-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194ebdc",
   "metadata": {},
   "source": [
    "#### Default Better Principle: Keep a hash of your data and check it\n",
    "To see what happens with a saved cached copy, load it again. Note the hash check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9581d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('penguins-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd91fe",
   "metadata": {},
   "source": [
    "The `penguins-raw` dataset contains information on where to find a single raw data file: `penguins_size.csv`. This is the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d109093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede67502",
   "metadata": {},
   "source": [
    "Normally, the hash is stored with the metadata and checked into the Dataset catalogue. Technically, it is here, with the hash of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70236e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.HASHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b99fe",
   "metadata": {},
   "source": [
    "For files, `.EXTRA` contains the hash information. We have the choice to store the `size` as the hash to keep giant data stores more manageable. We could also choose to use a hash function instead of `size` if we wanted to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40aa5cd",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png\" alt=\"Diagram of culmen measurements on a penguin\" width=300px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9c3e4",
   "metadata": {},
   "source": [
    "Without going in to the details, `.EXTRA` lets us manage files off of our local system if necessary and keeps track of raw data file details for our Dataset recipes. To get the fully qualified path to `penguins_size.csv` or any `.EXTRA` file, we can use \n",
    "\n",
    "`.EXTRA` will always keep track of relative paths so you are never checking in anything except relative paths to your repo, while using your local paths to resolve where to find the files via `.extra_file()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.extra_file('penguins_size.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b40a0",
   "metadata": {},
   "source": [
    "Let's start looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(ds.extra_file('penguins_size.csv'))\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cac47",
   "metadata": {},
   "source": [
    "First up, we will get rid of the NAs in\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef455ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "penguins.species_short.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5334b5e",
   "metadata": {},
   "source": [
    "Visualizing this data is a little bit\n",
    "tricky since we can't plot in 4 dimensions easily. Fortunately four is\n",
    "not that large a number, so we can just to a pairwise feature\n",
    "scatterplot matrix to get an ideas of what is going on. Seaborn makes\n",
    "this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguins, hue='species_short');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25eb65",
   "metadata": {},
   "source": [
    "\n",
    "Before we can do any work with the data it will help to clean up it a\n",
    "little. We won't need NAs, we just want the measurement columns, and\n",
    "since the measurements are on entirely different scales it will be\n",
    "helpful to convert each feature into z-scores (number of standard\n",
    "deviations from the mean) for comparability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9179fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_data = penguins[\n",
    "    [\n",
    "        \"culmen_length_mm\",\n",
    "        \"culmen_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "    ]\n",
    "].values\n",
    "scaled_penguin_data = StandardScaler().fit_transform(penguin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33021d4d",
   "metadata": {},
   "source": [
    "We're ready for our analysis. Since we don't want to have to do this work again repeatedly, this is a nice place to save our work (i.e. let Easydata remember the recipe for us), and have the notebook here as a handy reference still if we want to refer back to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c68bd",
   "metadata": {},
   "source": [
    "## Save the new datasets using this Notebook as a Transformer\n",
    "\n",
    "#### Default Better Principle: Keep easy-to-use data recipes\n",
    "\n",
    "We will now save new data as a derived Dataset, say `penguins-clean` and `penguins-scaled`, so that we can access the data from anywhere this repo is installed via `Dataset.load()`. For our purposes that will be the next notebook! We'll do it in another notebook to separate the grungy prep work from the analysis itself.\n",
    "\n",
    "Easydata uses Dataset Transformers and these Transformers can work as one-to-one, one-to-many, and even many-to-many. For a Notebook as a Transformer, the code of this Notebook acts as the transformer. The input datasets as the Datasets the we depend on in the Notebook via `Dataset.load()` to run, and the ouputs are the Datasets that we create in the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset_name = \"penguins-scaled\"\n",
    "scaled_data = scaled_penguin_data\n",
    "scaled_metadata = ds.metadata.copy() # start with a copy of the same metadata\n",
    "\n",
    "# add some new metadata description reflecting what we did\n",
    "scaled_descr_txt = f\"\"\"\\n\\nData cleaned up by removing NAs and scaling. See notebook 04.\"\"\"\n",
    "\n",
    "scaled_metadata['descr'] += scaled_descr_txt\n",
    "\n",
    "scaled_ds = Dataset(dataset_name=scaled_dataset_name, data=scaled_data,\n",
    "                 metadata=scaled_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset_name = \"penguins-clean\"\n",
    "clean_data = penguins\n",
    "clean_metadata = ds.metadata.copy() # start with a fresh copy of the same metadata\n",
    "\n",
    "\n",
    "# add some new metadata description reflecting what we did\n",
    "clean_descr_txt = f\"\"\"\\n\\nData cleaned up by removing NAs. See notebook 04.\"\"\"\n",
    "\n",
    "clean_metadata['descr'] += clean_descr_txt\n",
    "\n",
    "clean_ds = Dataset(dataset_name=clean_dataset_name, data=clean_data,\n",
    "                 metadata=clean_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to various design choiced in Jupyter, we need to specify this name manually.\n",
    "nbname = '04-Data-Challenge.ipynb'\n",
    "dsdict = notebook_as_transformer(notebook_name=nbname,\n",
    "                                 input_datasets=[ds],\n",
    "                                 output_datasets=[scaled_ds, clean_ds],\n",
    "                                 overwrite_catalog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc4792",
   "metadata": {},
   "source": [
    "Check that we can now load our new dataset from the Dataset Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = Dataset.load(\"penguins-scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812199d",
   "metadata": {},
   "source": [
    "Let's check that `.data` is the same as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_ds.data == scaled_penguin_data).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47464ed",
   "metadata": {},
   "source": [
    "And that the `.DESCR` text has been updated the way we stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a21c36",
   "metadata": {},
   "source": [
    "Finally, let's check that the license passed on appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30793d40",
   "metadata": {},
   "source": [
    "The Catalog is managed by git. You should see some new, non-checked in files in your catalog related to...you guessed it...penguins! Check them in now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28b507",
   "metadata": {},
   "source": [
    "#### Default Better Principle: Data is immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e7bec",
   "metadata": {},
   "source": [
    "Now when you come back and want to use this dataset, all it takes is running `Dataset.load('penguins-clean')`.\n",
    "\n",
    "Everything will just work. In our example with our colleague at the beginning, as long as we still had the `data/raw` file lying around, we would have had everything that we needed to reproduce the work, as is (especially since we aleard included the metadata the first time). \n",
    "\n",
    "Say your colleague ran the recipe, but got a different download (as expected), they would have a different hash for the raw data file, you could compare hashes and go from there. There are two options at that point.\n",
    "1. **Reproduce the existing work**: As long as you have the right to redistribute the dataset (as in this example with CC-0), you could send the raw data file to your colleague, or host it in another way (and update the Dataset recipe accordingly via manual download or a new URL). That means you can recreate the exact work with no more issues. You can either stay here, or move on to accomodate the new data.\n",
    "2. **Compare the new and old results**: If you decide to try and work with the new data, create a new recipe for the new data, and go from there. This will also allow you to compare your results with your old work.\n",
    "\n",
    "Overall, you'll see where things have changed right away, you'll know where to look for the differences, and you won't have lost your ability to recreate your existing work by still having a copy of the raw original data file around.\n",
    "\n",
    "Reproducibility winning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c06e1",
   "metadata": {},
   "source": [
    "## Complete the challenge\n",
    "Let's make sure that the data recipes work and that this Notebook runs to completion.\n",
    "\n",
    "Run `make data_challenge` to check that you've completed the challenge and to continue with your reproducibility quest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:easydata-tutorial] *",
   "language": "python",
   "name": "conda-env-easydata-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
