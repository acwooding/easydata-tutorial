{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c3d6e7",
   "metadata": {},
   "source": [
    "# Stage 4: Where do I find the data?\n",
    "\n",
    "This is data science and data analysis after all. We need data. And when we have data, we need to know what data we have. Here are the big questions around finding and prepping data:\n",
    "* Where do I find the data?\n",
    "* How can I use the data?\n",
    "* How do I know that I have the right data?\n",
    "* **BONUS** How can I make it easy to retrieve and use again? (Without copy and pasting code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446c591",
   "metadata": {},
   "source": [
    "### Imagine: a hypothetical workflow for downloaded data \n",
    "(Assuming you're using reproducible environments already!)\n",
    "\n",
    "1. **Downloading data**: First, you look for a dataset to use. You get one. You download the data, or you copy and paste a script you find off of the internet for how to download the data. \n",
    "2. **Data prep**: Then you copy and paste the munging script that you saw for how to get it into a pandas dataframe.\n",
    "3. **Data prep-2**: You try some analysis, but it doesn't quite work, you need to clean the data up a bit more, so you go back to doing some more prep, this time, your own way. \n",
    "4. **Analysis**: Finally, you can run your analysis and do some data science work. \n",
    "5. ...time passes...\n",
    "6. **Re-downloading**: You mention the result and a colleague asks you about the work, so you go back to dig up the code. You decide to see if you can share your work. You start to run your code, but then realize that the dataset has been updated. It's not clear what's been changed, but you notice in the output that there seem to be more entries in the dataframe. When your script downloaded it, you replaced the previous dataset with the new one. \n",
    "7. **Checking licenses**: When you go to try and figure out what happened, and start looking up info on the dataset. The source you used for information the first time doesn't seem to be up-to-date so you keep digging until you find the original source. In the process, you realize you forgot to look at the license. Better check on that before sharing your work. Thankfully, it's CC-BY-NC, and you're not using it for work purposes. You take a sigh of relief, as this could have been a show-stopper, and you continue.\n",
    "8. **Data prep-3**: You can't roll back to the old data, and the new data has some new gotchas, so you spend some time trying to sort that out. Finally you have something that you can re-run your analysis on, and share with your colleague. \n",
    "9. **Analysis-2**: Fingers crossed with the new larger dataset that things work our roughly the same. There's no easy way now to figure out what's going on if the results look different from before...\n",
    "10. **Sharing**: Phew. Wiping the sweat from your brow, things look roughly similar. You can pass on the work to your colleague.\n",
    "\n",
    "In the hypothetical (or not...this may be pulling from personal experience) situation above, getting to the analysis stage the second time around took just as long, if not longer because of trying to figure out what had changed about the dataset, and tracking down the dataset information and licenses, which were no longer as easy to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d79d1",
   "metadata": {},
   "source": [
    "### Some issues\n",
    "In the previous workflow, here were some of the issues that made it hard to reproduce:\n",
    "\n",
    "* **NO-DATA-LICENSE**: No data license with the data. In fact, we forgot to even check the first time. It wasn't a showstopper, this time, but it could have easily been.\n",
    "* **NO-DATA-METADATA**: No metadata with the data. We didn't have a description of what the data was in a way that made it easy to see what the data was. And in this case, what might have changed with the new metadata.\n",
    "* **NO-DATA-HASH**: No data hash. This meant that there was no way to catch early on that the data was different. \n",
    "* **COOKED-DATA**: Data gets cooked when it gets changed. In this case, the change was downloading the new data on top of the old. Most frequently, we see this happening when data that gets preped gets saved over the raw data. Repeat after me: \"Data is immutable\".\n",
    "* **BLACK-MAGIC**: The data prep was a form of black magic. In this example, we copied and pasted, and we didn't pay attention to what was inside it, it just worked the first time. In particular, the download script had a hard-coded path that the download went to and it would redownload the file into the same place every time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2d113",
   "metadata": {},
   "source": [
    "## The Easydata Way\n",
    "\n",
    "### Principles\n",
    "\n",
    "* **Data and metadata always stay together**: Keep your data and your metadata together at all times. This includes licenses and hashes. \n",
    "* **Keep a hash of your data and check it**: This is especially true for your raw data, your starting points from which the rest of your data is derived.\n",
    "* **Data is immutable**: Never edit a raw data file. Especially not manually. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. \n",
    "* **Keep (easy-to-use) data recipes**: The code you write should move the raw data through a pipeline to your final analysis. Keep data recipes as a way of recovering your data from its raw format. Your data recipe ideally has an easy-to-use API that gives people you're sharing with a *common start line* when working with the data.\n",
    "\n",
    "### How it's done in Easydata\n",
    "\n",
    "Now we'll take a look at how we do this in Easydata. First, some imports that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2282502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215105d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "import logging\n",
    "from src.log import logger\n",
    "from src.helpers import notebook_as_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafebc67",
   "metadata": {},
   "source": [
    "### Set to debug log level\n",
    "Let's set the log level on to DEBUG on the Easydata src module to see what the Dataset code is doing under the hood..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ada35b",
   "metadata": {},
   "source": [
    "## Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load(\"dataset-challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b10dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd04e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fb3e7",
   "metadata": {},
   "source": [
    "Penguin data\n",
    "------------\n",
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png\" alt=\"Penguins\" width=400px/>\n",
    "\n",
    "Time to try it out. The next step is to get some data to work with. To ease us into things\n",
    "we'll start with the [penguin\n",
    "dataset](https://github.com/allisonhorst/penguins). It isn't very\n",
    "representative of what real data would look like, but it is small both\n",
    "in number of points and number of features, and will let us quickly and easily get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019c715",
   "metadata": {},
   "source": [
    "## Load the pre-created a Dataset recipe\n",
    "\n",
    "Here we use the dataset recipe to get the raw penguins dataset. When running \n",
    "\n",
    "```ds = Dataset.load('penguins-raw')```\n",
    "\n",
    "Easydata uses an entry in the Dataset Catalog to find the `penguins-raw` Dataset recipe.\n",
    "\n",
    "Here's what it will do:\n",
    "* Look in the Catalog for the entry `penguins-raw`\n",
    "* Traceback through the dependency chain of datasets looking for what's already cashed and matches the hash check, and what needs to be created from scratch\n",
    "* If this is your first time running, it will download the dataset and created it the first time, populating the `data/raw` directory with the raw download\n",
    "* Do a hash check\n",
    "* Create the `penguins-raw` Dataset\n",
    "* If successful, it will save a processed version of the data in `data/processed` for quick and easy loading any time\n",
    "\n",
    "Say you need more space on your machine. You can blow away the contents of your `data/processed` directory at any time, and `.load(\"penguins-raw\")` will recreate the Dataset again from `data/raw` whenever you need to use it.\n",
    "\n",
    "**BONUS** Beyond the scope of this tutorial, your `data/raw` directory doesn't have to be local and can be kept on a bigger machine or even in your cloud storage (AWS, Azure, etc.). This can be handled by the local paths structure introduced in `02-Environment-Challenge`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff925e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('penguins-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c41a40",
   "metadata": {},
   "source": [
    "To see what happens with a saved cached copy, load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('penguins-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2308c",
   "metadata": {},
   "source": [
    "The `penguins-raw` dataset contains information on where to find a single raw data file: `penguins_size.csv`. This is the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d109093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40aa5cd",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png\" alt=\"Diagram of culmen measurements on a penguin\" width=300px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5a5d0",
   "metadata": {},
   "source": [
    "Without going in to the details, `.EXTRA` lets us manage files off of our local system if necessary and keeps track of raw data file details for our Dataset recipes. To get the fully qualified path to `penguins_size.csv` or any `.EXTRA` file, we can use \n",
    "\n",
    "`.EXTRA` will always keep track of relative paths so you are never checking in anything except relative paths to your repo, while using your local paths to resolve where to find the files via `.extra_file()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.extra_file('penguins_size.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904dc10",
   "metadata": {},
   "source": [
    "Let's start looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(ds.extra_file('penguins_size.csv'))\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cac47",
   "metadata": {},
   "source": [
    "First up, we will get rid of the NAs in\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef455ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "penguins.species_short.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5334b5e",
   "metadata": {},
   "source": [
    "Visualizing this data is a little bit\n",
    "tricky since we can't plot in 4 dimensions easily. Fortunately four is\n",
    "not that large a number, so we can just to a pairwise feature\n",
    "scatterplot matrix to get an ideas of what is going on. Seaborn makes\n",
    "this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguins, hue='species_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25eb65",
   "metadata": {},
   "source": [
    "\n",
    "Before we can do any work with the data it will help to clean up it a\n",
    "little. We won't need NAs, we just want the measurement columns, and\n",
    "since the measurements are on entirely different scales it will be\n",
    "helpful to convert each feature into z-scores (number of standard\n",
    "deviations from the mean) for comparability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9179fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_data = penguins[\n",
    "    [\n",
    "        \"culmen_length_mm\",\n",
    "        \"culmen_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "    ]\n",
    "].values\n",
    "scaled_penguin_data = StandardScaler().fit_transform(penguin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33021d4d",
   "metadata": {},
   "source": [
    "We're ready for our analysis. Since we don't want to have to do this work again repeatedly, this is a nice place to save our work (i.e. let Easydata remember the recipe for us), and have the notebook here as a handy reference still if we want to refer back to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c68bd",
   "metadata": {},
   "source": [
    "## Save the new dataset using this Notebook as a Transformer\n",
    "\n",
    "We will now save new data as a derived Dataset, say `penguins-clean`, so that we can access the data from anywhere this repo is installed via `Dataset.load(\"penguins-clean\")`. For our purposes that will be the next notebook! We'll do it in another notebook to separate the grungy prep work from the analysis itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_name = \"penguins-clean\"\n",
    "new_data = scaled_penguin_data\n",
    "new_metadata = ds.metadata.copy() # start with the same metadata\n",
    "\n",
    "# add some new metadata description reflecting what we did\n",
    "added_descr_txt = f\"\"\"\\n\\nData cleaned up by removing NAs and scaling. See notebook 04.\"\"\"\n",
    "\n",
    "new_metadata['descr'] += added_descr_txt\n",
    "\n",
    "new_ds = Dataset(dataset_name=new_dataset_name, data=new_data,\n",
    "                 metadata=new_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to various design choiced in Jupyter, we need to specify this name manually.\n",
    "nbname = '04-Data-Challenge.ipynb'\n",
    "dsdict = notebook_as_transformer(notebook_name=nbname,\n",
    "                                 input_datasets=[ds],\n",
    "                                 output_datasets=[new_ds],\n",
    "                                 overwrite_catalog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc4792",
   "metadata": {},
   "source": [
    "Check that we can now load our new dataset from the Dataset Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = Dataset.load(\"penguins-clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca23658",
   "metadata": {},
   "source": [
    "Let's check that `.data` is the same as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_ds.data == scaled_penguin_data).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6a785",
   "metadata": {},
   "source": [
    "And that the `.DESCR` text has been updated the way we stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_ds.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ba1cd",
   "metadata": {},
   "source": [
    "Finally, let's check that the license passed on appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_ds.LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.quest import data_test\n",
    "## Test that we can ds.load the penguins-clean dataset\n",
    "## Test that it has the expected metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beafd66",
   "metadata": {},
   "source": [
    "for the data_test text\n",
    "Now when you come back and want to use this dataset, all it takes is running `Dataset.load('penguins-clean')`.\n",
    "\n",
    "Everything will just work. In our example with our colleague at the beginning, as long as we still had the `data/raw` file lying around, we would have had everything that we needed to reproduce the work, as is (especially since we aleard included the metadata the first time). \n",
    "\n",
    "Say your colleague ran the recipe, but got a different download (as expected), they would have a different hash for the raw data file, you could compare hashes and go from there. There are two options at that point.\n",
    "1. **Reproduce the existing work**: As long as you have the right to redistribute the dataset (as in this example with CC-0), you could send the raw data file to your colleague, or host it in another way (and update the Dataset recipe accordingly via manual download or a new URL). That means you can recreate the exact work with no more issues. You can either stay here, or move on to accomodate the new data.\n",
    "2. **Compare the new and old results**: If you decide to try and work with the new data, create a new recipe for the new data, and go from there. This will also allow you to compare your results with your old work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afeda2e",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* Datasets: https://cookiecutter-easydata.readthedocs.io/en/latest/datasets/\n",
    "* Point to read the docs for how to create a dataset from scratch: https://cookiecutter-easydata.readthedocs.io/en/latest/New-Dataset-Template/\n",
    "* Create a Penguins Dataset: https://github.com/allisonhorst/palmerpenguins\t\t\n",
    "* The reference notebooks for dataset creation\n",
    "* Dataset DAG post\n",
    "* Local paths...in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc90c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:easydata-tutorial] *",
   "language": "python",
   "name": "conda-env-easydata-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
